"""
LUMINARK Dashboard Visual Tester
Automated UI/UX validation for the Logistics Dashboard
Inspired by DeepAgent's autonomous testing
"""

import asyncio
import json
from datetime import datetime
from typing import Dict, List

class DashboardTester:
    """
    Automated testing for the Logistics Dashboard
    Validates API responses and UI data flow
    """
    
    def __init__(self):
        self.test_results = []
        self.api_endpoint = "http://localhost:8000/api/analyze"
        
    def generate_test_vectors(self) -> List[Dict]:
        """
        Generate test input vectors for the dashboard
        Covers various risk scenarios
        """
        return [
            {
                'name': 'LOW_RISK_SCENARIO',
                'description': 'Perfect conditions - should show green/safe',
                'vectors': {
                    'weather': 0.1,
                    'traffic': 0.1,
                    'driver_fatigue': 0.1,
                    'cargo_value': 0.2,
                    'route_complexity': 0.1
                },
                'expected_stage_range': (1, 4),
                'expected_risk': 'LOW'
            },
            {
                'name': 'MEDIUM_RISK_SCENARIO',
                'description': 'Moderate concerns - should show yellow/caution',
                'vectors': {
                    'weather': 0.5,
                    'traffic': 0.5,
                    'driver_fatigue': 0.4,
                    'cargo_value': 0.6,
                    'route_complexity': 0.5
                },
                'expected_stage_range': (4, 7),
                'expected_risk': 'MEDIUM'
            },
            {
                'name': 'HIGH_RISK_SCENARIO',
                'description': 'Dangerous conditions - should show red/alert',
                'vectors': {
                    'weather': 0.9,
                    'traffic': 0.8,
                    'driver_fatigue': 0.9,
                    'cargo_value': 0.9,
                    'route_complexity': 0.8
                },
                'expected_stage_range': (7, 10),
                'expected_risk': 'HIGH'
            },
            {
                'name': 'EDGE_CASE_ALL_ZEROS',
                'description': 'All zeros - system should handle gracefully',
                'vectors': {
                    'weather': 0.0,
                    'traffic': 0.0,
                    'driver_fatigue': 0.0,
                    'cargo_value': 0.0,
                    'route_complexity': 0.0
                },
                'expected_stage_range': (1, 3),
                'expected_risk': 'LOW'
            },
            {
                'name': 'EDGE_CASE_ALL_MAX',
                'description': 'All maximum - system should handle gracefully',
                'vectors': {
                    'weather': 1.0,
                    'traffic': 1.0,
                    'driver_fatigue': 1.0,
                    'cargo_value': 1.0,
                    'route_complexity': 1.0
                },
                'expected_stage_range': (8, 10),
                'expected_risk': 'CRITICAL'
            },
            {
                'name': 'YUNUS_TRIGGER_TEST',
                'description': 'High stage + high risk - should trigger Yunus monitoring',
                'vectors': {
                    'weather': 0.85,
                    'traffic': 0.85,
                    'driver_fatigue': 0.85,
                    'cargo_value': 0.85,
                    'route_complexity': 0.85
                },
                'expected_stage_range': (7, 10),
                'expected_risk': 'HIGH',
                'expect_yunus': True
            }
        ]
    
    async def test_api_response(self, test_case: Dict) -> Dict:
        """
        Test a single scenario against the API
        Returns validation results
        """
        import requests
        
        try:
            # Make API call
            response = requests.post(
                self.api_endpoint,
                json=test_case['vectors'],
                timeout=5
            )
            
            if response.status_code != 200:
                return {
                    'test_case': test_case['name'],
                    'passed': False,
                    'error': f"HTTP {response.status_code}",
                    'timestamp': datetime.now().isoformat()
                }
            
            data = response.json()
            
            # Validate response structure
            required_fields = ['stage', 'stage_name', 'risk_level', 'recommendations', 'safety_report']
            missing_fields = [f for f in required_fields if f not in data]
            
            if missing_fields:
                return {
                    'test_case': test_case['name'],
                    'passed': False,
                    'error': f"Missing fields: {missing_fields}",
                    'timestamp': datetime.now().isoformat()
                }
            
            # Validate stage range
            stage = data['stage']
            expected_min, expected_max = test_case['expected_stage_range']
            stage_valid = expected_min <= stage <= expected_max
            
            # Validate risk level
            risk_valid = True
            if 'expected_risk' in test_case:
                expected_risks = test_case['expected_risk'].split('|')
                risk_valid = data['risk_level'] in expected_risks or data['risk_level'] in ['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']
            
            # Check Yunus activation if expected
            yunus_valid = True
            if test_case.get('expect_yunus', False):
                yunus_valid = 'yunus' in data.get('safety_report', {}).get('active_protocols', [])
            
            # Overall validation
            passed = stage_valid and risk_valid and yunus_valid
            
            return {
                'test_case': test_case['name'],
                'description': test_case['description'],
                'passed': passed,
                'response': data,
                'validations': {
                    'stage_valid': stage_valid,
                    'stage_actual': stage,
                    'stage_expected': f"{expected_min}-{expected_max}",
                    'risk_valid': risk_valid,
                    'risk_actual': data['risk_level'],
                    'risk_expected': test_case.get('expected_risk', 'ANY'),
                    'yunus_valid': yunus_valid
                },
                'timestamp': datetime.now().isoformat()
            }
            
        except requests.exceptions.ConnectionError:
            return {
                'test_case': test_case['name'],
                'passed': False,
                'error': "Cannot connect to API - is the server running?",
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            return {
                'test_case': test_case['name'],
                'passed': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    async def run_all_tests(self) -> Dict:
        """
        Run complete dashboard test suite
        """
        print("üé® LUMINARK Dashboard Tester")
        print("=" * 70)
        print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Target: {self.api_endpoint}")
        print()
        
        test_cases = self.generate_test_vectors()
        
        print(f"Running {len(test_cases)} dashboard test scenarios...")
        print()
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"[{i}/{len(test_cases)}] Testing: {test_case['description']}")
            result = await self.test_api_response(test_case)
            self.test_results.append(result)
            
            # Show result
            if result['passed']:
                print(f"    ‚úÖ PASS")
            else:
                print(f"    ‚ùå FAIL")
                if 'error' in result:
                    print(f"    Error: {result['error']}")
                elif 'validations' in result:
                    v = result['validations']
                    if not v['stage_valid']:
                        print(f"    Stage: Expected {v['stage_expected']}, Got {v['stage_actual']}")
                    if not v['risk_valid']:
                        print(f"    Risk: Expected {v['risk_expected']}, Got {v['risk_actual']}")
            
            print()
        
        return self.generate_report()
    
    def generate_report(self) -> Dict:
        """
        Generate comprehensive dashboard test report
        """
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r['passed'])
        failed_tests = total_tests - passed_tests
        
        # Connection check
        connection_errors = sum(1 for r in self.test_results if 'error' in r and 'connect' in r.get('error', '').lower())
        
        report = {
            'summary': {
                'total_tests': total_tests,
                'passed': passed_tests,
                'failed': failed_tests,
                'success_rate': (passed_tests / total_tests * 100) if total_tests > 0 else 0,
                'connection_errors': connection_errors,
                'timestamp': datetime.now().isoformat()
            },
            'test_results': self.test_results,
            'recommendations': []
        }
        
        # Generate recommendations
        if connection_errors > 0:
            report['recommendations'].append({
                'priority': 'CRITICAL',
                'message': 'API server is not running. Start with: start_logistics.bat'
            })
        
        if failed_tests > 0 and connection_errors == 0:
            report['recommendations'].append({
                'priority': 'HIGH',
                'message': f'{failed_tests} test(s) failed validation. Review response data.'
            })
        
        if passed_tests == total_tests:
            report['recommendations'].append({
                'priority': 'INFO',
                'message': 'üéâ All tests passed! Dashboard is demo-ready.'
            })
        
        return report
    
    def print_report(self, report: Dict):
        """
        Print formatted dashboard test report
        """
        print("\n" + "=" * 70)
        print("üìä DASHBOARD TEST REPORT")
        print("=" * 70)
        
        summary = report['summary']
        print(f"\n‚úÖ Tests Passed: {summary['passed']}/{summary['total_tests']}")
        print(f"‚ùå Tests Failed: {summary['failed']}/{summary['total_tests']}")
        print(f"üìà Success Rate: {summary['success_rate']:.1f}%")
        
        if summary['connection_errors'] > 0:
            print(f"\n‚ö†Ô∏è  Connection Errors: {summary['connection_errors']}")
            print("   Make sure to run: start_logistics.bat")
        
        if report['recommendations']:
            print("\n" + "-" * 70)
            print("üí° RECOMMENDATIONS")
            print("-" * 70)
            for rec in report['recommendations']:
                icon = "üî¥" if rec['priority'] == 'CRITICAL' else "üü°" if rec['priority'] == 'HIGH' else "üü¢"
                print(f"{icon} [{rec['priority']}] {rec['message']}")
        
        print("\n" + "=" * 70)
        
        # Final verdict
        if summary['connection_errors'] > 0:
            print("‚ö†Ô∏è  START THE SERVER: Run start_logistics.bat first!")
        elif summary['success_rate'] == 100:
            print("üéâ DASHBOARD IS PERFECT! Ready for Nikki's demo!")
        elif summary['success_rate'] >= 80:
            print("‚úÖ DASHBOARD IS GOOD! Minor issues to review.")
        else:
            print("‚ö†Ô∏è  DASHBOARD NEEDS ATTENTION! Review failures above.")
        print("=" * 70 + "\n")
    
    def save_report(self, report: Dict, filename: str = "dashboard_test_report.json"):
        """
        Save report to JSON file
        """
        with open(filename, 'w') as f:
            json.dump(report, f, indent=2)
        print(f"üìÑ Report saved to: {filename}")

async def main():
    """
    Main dashboard test execution
    """
    tester = DashboardTester()
    report = await tester.run_all_tests()
    tester.print_report(report)
    tester.save_report(report, "c:\\Users\\Forev\\OneDrive\\Documents\\GitHub\\LUMINARK\\dashboard_test_report.json")
    
    return report

if __name__ == "__main__":
    asyncio.run(main())
